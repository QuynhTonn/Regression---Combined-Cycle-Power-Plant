{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc =SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-------+-----+------+\n",
      "|   AT|    V|     AP|   RH|    PE|\n",
      "+-----+-----+-------+-----+------+\n",
      "|14.96|41.76|1024.07|73.17|463.26|\n",
      "|25.18|62.96|1020.04|59.08|444.37|\n",
      "| 5.11| 39.4|1012.16|92.14|488.56|\n",
      "+-----+-----+-------+-----+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel('Du lieu cung cap/CCPP/Folds5x2_pp.ods')\n",
    "df = spark.createDataFrame(df)\n",
    "\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9568"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AT', 'V', 'AP', 'RH', 'PE']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(\n",
    "            inputCols = ['AT', 'V', 'AP', 'RH'],\n",
    "            outputCol = 'features') #input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pre = assembler.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-------+-----+------+--------------------+\n",
      "|   AT|    V|     AP|   RH|    PE|            features|\n",
      "+-----+-----+-------+-----+------+--------------------+\n",
      "|14.96|41.76|1024.07|73.17|463.26|[14.96,41.76,1024...|\n",
      "|25.18|62.96|1020.04|59.08|444.37|[25.18,62.96,1020...|\n",
      "+-----+-----+-------+-----+------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_pre.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+\n",
      "|features                   |\n",
      "+---------------------------+\n",
      "|[14.96,41.76,1024.07,73.17]|\n",
      "|[25.18,62.96,1020.04,59.08]|\n",
      "+---------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_pre.select(\"features\").show(2,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"ScaledFeatures\")\n",
    "# , withStd=True, withMean=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = scaler.fit(data_pre).transform(data_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9568"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9568"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data = final_data.na.drop()\n",
    "final_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-------+-----+------+--------------------+--------------------+\n",
      "|   AT|    V|     AP|   RH|    PE|            features|      ScaledFeatures|\n",
      "+-----+-----+-------+-----+------+--------------------+--------------------+\n",
      "|14.96|41.76|1024.07|73.17|463.26|[14.96,41.76,1024...|[0.37252124645892...|\n",
      "|25.18|62.96|1020.04|59.08|444.37|[25.18,62.96,1020...|[0.66203966005665...|\n",
      "| 5.11| 39.4|1012.16|92.14|488.56|[5.11,39.4,1012.1...|[0.09348441926345...|\n",
      "|20.86|57.32|1010.24|76.64|446.48|[20.86,57.32,1010...|[0.53966005665722...|\n",
      "|10.82| 37.5|1009.23|96.62| 473.9|[10.82,37.5,1009....|[0.25524079320113...|\n",
      "+-----+-----+-------+-----+------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = final_data.select('ScaledFeatures','PE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|                PE|\n",
      "+-------+------------------+\n",
      "|  count|              7687|\n",
      "|   mean| 454.1700741511647|\n",
      "| stddev|17.031163025671304|\n",
      "|    min|            421.57|\n",
      "|    max|            495.76|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Chia tập dữ liệu\n",
    "train_data, test_data = final_data.randomSplit([0.8,0.2])\n",
    "train_data.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|                PE|\n",
      "+-------+------------------+\n",
      "|  count|              1881|\n",
      "|   mean|455.16164274322176|\n",
      "| stddev|17.194243773684697|\n",
      "|    min|            420.26|\n",
      "|    max|            495.21|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_data.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "from pyspark.ml.regression import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression(featuresCol = \"ScaledFeatures\",\n",
    "                      labelCol = \"PE\",\n",
    "                      predictionCol = \"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrModel = lr.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficiens: [-69.32167095942808,-13.128799893031823,2.7257279763784066,-11.663991490246497] Intercept: 502.2293576277128\n"
     ]
    }
   ],
   "source": [
    "print(\"Coefficiens: {} Intercept: {}\" .format(lrModel.coefficients, lrModel.intercept))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result = lrModel.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|          residuals|\n",
      "+-------------------+\n",
      "| -5.027167434374746|\n",
      "|-0.4985496383010286|\n",
      "|  2.852908279108817|\n",
      "|  7.116460376241719|\n",
      "|-1.4745726761236142|\n",
      "+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_result.residuals.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 4.408877540754137\n",
      "MSE: 19.43820116936625\n",
      "r2: 0.9342159118408833\n"
     ]
    }
   ],
   "source": [
    "print(\"RMSE: {}\" .format(test_result.rootMeanSquaredError))\n",
    "print(\"MSE: {}\" .format(test_result.meanSquaredError))\n",
    "print(\"r2: {}\" .format(test_result.r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------+\n",
      "|        prediction|    PE|\n",
      "+------------------+------+\n",
      "| 486.3071674343747|481.28|\n",
      "|483.47854963830105|482.98|\n",
      "|487.10709172089116|489.96|\n",
      "| 481.9335396237583|489.05|\n",
      "|486.38457267612364|484.91|\n",
      "+------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check test dataset\n",
    "test_model = lrModel.transform(test_data)\n",
    "#Inspect results\n",
    "test_model.select(\"prediction\", \"PE\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "lrModel.save('lrModel_Cau3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict with new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegressionModel\n",
    "# # Load model\n",
    "lrModel2 = LinearRegressionModel.load('lrModel_Cau3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-------+-----+------+\n",
      "|   AT|    V|     AP|   RH|    PE|\n",
      "+-----+-----+-------+-----+------+\n",
      "| 9.44| 40.0|1015.62|81.16|471.32|\n",
      "|23.49| 49.3|1003.35|77.96|442.76|\n",
      "| 4.99|39.04|1020.45|78.89|472.52|\n",
      "+-----+-----+-------+-----+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_new = pd.read_excel('Du lieu cung cap/CCPP/Folds5x2_pp.ods',sheet_name=2)\n",
    "df_new = spark.createDataFrame(df_new)\n",
    "\n",
    "df_new.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "data_pre_new = assembler.transform(df_new)\n",
    "# Scale data\n",
    "final_data_new = scaler.fit(data_pre).transform(data_pre)\n",
    "final_data_new = final_data.select('ScaledFeatures','PE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------+\n",
      "|        prediction|    PE|\n",
      "+------------------+------+\n",
      "| 467.2335268609262|463.26|\n",
      "| 444.1423194680943|444.37|\n",
      "| 483.3587598977769|488.56|\n",
      "|450.53683014847707|446.48|\n",
      "|471.69129526017514| 473.9|\n",
      "+------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = lrModel.transform(final_data_new)\n",
    "#Inspect results\n",
    "result.select(\"prediction\", \"PE\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
